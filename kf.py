# -*- coding: utf-8 -*-
"""entrenado desde UAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MkWqPJXFzxsJOJMN7ZrwjyHLrHOyoF5g
"""

!pip install pandas nltk transformers matplotlib pyspark emoji textblob openpyxl

import nltk
import emoji
import string
import re
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from transformers import pipeline
from wordcloud import WordCloud
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from collections import Counter
from google.colab import files

# Crear SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

try:
    nltk.download('vader_lexicon')
    nltk.download('punkt')
except Exception as e:
    print("Error al descargar recursos de NLTK:", e)

# Cargar el modelo BERT para espa침ol
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
# Funci칩n para an치lisis de sentimientos con NLTK
def analizar_sentimiento_nltk(texto):
    sia = SentimentIntensityAnalyzer()
    texto_modificado = texto.replace("ja", "feliz").replace("XD", "alegre")
    puntaje = sia.polarity_scores(texto_modificado)
    return 'positivo' if puntaje['compound'] > 0.05 else ('negativo' if puntaje['compound'] < -0.05 else 'neutro')
# Funci칩n para an치lisis de sentimientos con BERT
def analizar_sentimiento_bert(texto):
    resultado = sentiment_pipeline(texto)[0]
    return resultado['label']
# UDFs para Spark
analizar_sentimiento_nltk_udf = udf(analizar_sentimiento_nltk, StringType())
analizar_sentimiento_bert_udf = udf(analizar_sentimiento_bert, StringType())

# Subir archivo
uploaded = files.upload()
nombre_archivo = next(iter(uploaded))

# Leer el archivo xls en un DataFrame de Spark
df_pandas = pd.read_excel(nombre_archivo)

# Funci칩n para extraer emojis de un texto usando una expresi칩n regular
def extraer_emojis(texto):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return ' '.join(emoji_pattern.findall(texto))

# Aplicar la funci칩n para extraer emojis a cada comentario
df_pandas['emojis'] = df_pandas['comentario'].apply(extraer_emojis)

# Crear un DataFrame solo con los comentarios y los emojis extra칤dos
df_emojis = df_pandas[['comentario', 'emojis']]

# Filtrar para mostrar solo filas donde se encontraron emojis
df_emojis = df_emojis[df_emojis['emojis'].str.len() > 0]

# Mostrar el DataFrame resultante
print(df_emojis)

# Crear una lista con todos los emojis
lista_emojis = ' '.join(df_emojis['emojis']).split()

# Contar la frecuencia de cada emoji
conteo_emojis = Counter(lista_emojis)

# Convertir el DataFrame de Pandas a un DataFrame de Spark
df = spark.createDataFrame(df_pandas)

# Aplicar UDFs al DataFrame
df = df.withColumn("sentimiento_nltk", analizar_sentimiento_nltk_udf(col("comentario")))
df = df.withColumn("sentimiento_bert", analizar_sentimiento_bert_udf(col("comentario")))

df.show()

# Convertir a DataFrame de Pandas para facilitar la visualizaci칩n
df_pandas = df.toPandas()

# Contar sentimientos para NLTK y BERT
sentimientos_nltk = df_pandas['sentimiento_nltk'].value_counts()
sentimientos_bert = df_pandas['sentimiento_bert'].value_counts()

# Crear gr치ficos de barras
plt.figure(figsize=(12, 12))

# Gr치fico para NLTK
plt.subplot(1, 2, 1)
sentimientos_nltk.plot(kind='bar', color=['green', 'blue', 'red'])
plt.title('An치lisis de Sentimientos NLTK')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad')

# Gr치fico para BERT
plt.subplot(1, 2, 2)
sentimientos_bert.plot(kind='bar', color=['green', 'blue', 'red'])
plt.title('An치lisis de Sentimientos BERT')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad')


plt.tight_layout()
plt.show()

# Funci칩n para limpiar y tokenizar el texto
def limpiar_y_tokenizar(texto):
    # Convertir a min칰sculas
    texto = texto.lower()
    # Eliminar puntuaci칩n
    texto = texto.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar
    tokens = word_tokenize(texto)
    return tokens

# Aplicar la funci칩n de limpieza y tokenizaci칩n a cada comentario
tokens = df_pandas['comentario'].apply(limpiar_y_tokenizar)

# Contar la frecuencia de cada palabra
frecuencia_palabras = Counter()
for comentario in tokens:
    frecuencia_palabras.update(comentario)

# Convertir el conteo de palabras en un DataFrame de Pandas
df_frecuencia_palabras = pd.DataFrame(frecuencia_palabras.items(), columns=['Palabra', 'Frecuencia']).sort_values(by='Frecuencia', ascending=False)

# Mostrar las 20 palabras m치s comunes en un cuadro
df_frecuencia_palabras.head(20)

# Funci칩n para contar s칤labas en espa침ol (aproximaci칩n simple)
def contar_silabas(palabra):
    # Contar vocales en la palabra
    return sum(1 for letra in palabra if letra in "aeiou치칠칤칩칰")

# Filtrar palabras con m치s de dos s칤labas, excepto 'u'
palabras_filtradas = {palabra: frecuencia for palabra, frecuencia in frecuencia_palabras.items() if contar_silabas(palabra) > 2 or palabra == 'u'}

# Crear una nube de palabras
nube_palabras = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(palabras_filtradas)

# Mostrar la nube de palabras
plt.figure(figsize=(10, 5))
plt.imshow(nube_palabras, interpolation='bilinear')
plt.axis('off')
plt.show()

# Calcular la longitud de cada comentario
df_pandas['longitud'] = df_pandas['comentario'].apply(len)

# Crear gr치ficos de dispersi칩n
plt.figure(figsize=(14, 6))

# Gr치fico para NLTK
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_nltk')
plt.title('Longitud de Comentarios vs Sentimientos (NLTK)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

# Gr치fico para BERT
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_bert')
plt.title('Longitud de Comentarios vs Sentimientos (BERT)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

plt.tight_layout()
plt.show()

# Funci칩n modificada para an치lisis de sentimientos con BERT
def analizar_sentimiento_bert_modificado(texto):
    resultado = sentiment_pipeline(texto)[0]
    etiqueta = resultado['label']

    # Convertir la clasificaci칩n de estrellas a positivo/negativo/neutro
    if etiqueta in ["1 star", "2 stars"]:
        return "negativo"
    elif etiqueta == "3 stars":
        return "neutro"
    else:  # "4 stars" y "5 stars"
        return "positivo"

# Aplicar la funci칩n modificada al DataFrame
df_pandas['sentimiento_bert_modificado'] = df_pandas['comentario'].apply(analizar_sentimiento_bert_modificado)

# Crear gr치ficos de dispersi칩n
plt.figure(figsize=(14, 6))

# Gr치fico para NLTK
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_nltk')
plt.title('Longitud de Comentarios vs Sentimientos (NLTK)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

# Gr치fico para BERT modificado
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_bert_modificado')
plt.title('Longitud de Comentarios vs Sentimientos (BERT Modificado)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

plt.tight_layout()
plt.show()

# Categorizar la longitud de los comentarios
def categorizar_longitud(longitud):
    if longitud < 50:
        return 'Corto'
    elif longitud < 100:
        return 'Medio'
    else:
        return 'Largo'

df_pandas['categoria_longitud'] = df_pandas['longitud'].apply(categorizar_longitud)

# Contar sentimientos por categor칤a de longitud para NLTK y BERT
sentimientos_por_longitud_nltk = df_pandas.groupby(['categoria_longitud', 'sentimiento_nltk']).size().unstack().fillna(0)
sentimientos_por_longitud_bert = df_pandas.groupby(['categoria_longitud', 'sentimiento_bert']).size().unstack().fillna(0)

# Crear gr치ficos de barras apiladas
plt.figure(figsize=(14, 6))

# Gr치fico para NLTK
plt.subplot(1, 2, 1)
sentimientos_por_longitud_nltk.plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Sentimientos por Longitud de Comentarios (NLTK)')
plt.xlabel('Categor칤a de Longitud')
plt.ylabel('Cantidad')
plt.legend(title='Sentimiento')

# Gr치fico para BERT
plt.subplot(1, 2, 2)
sentimientos_por_longitud_bert.plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Sentimientos por Longitud de Comentarios (BERT)')
plt.xlabel('Categor칤a de Longitud')
plt.ylabel('Cantidad')
plt.legend(title='Sentimiento')

plt.tight_layout()
plt.show()

# Crear un nuevo DataFrame
bert_sentimientos = df_pandas[['comentario', 'sentimiento_bert_modificado']]

# Mostrar el nuevo DataFrame
print(bert_sentimientos)

# Convertir el conteo en un DataFrame
df_conteo_emojis = pd.DataFrame(conteo_emojis.items(), columns=['Emoji', 'Frecuencia'])

# Ordenar el DataFrame por frecuencia de manera descendente
df_conteo_emojis = df_conteo_emojis.sort_values(by='Frecuencia', ascending=False)

# Configurar pandas para mostrar todas las filas
pd.set_option('display.max_rows', None)

# Mostrar el DataFrame
print(df_conteo_emojis)

print(df_conteo_emojis)


categorias_emojis = {
    "游녪": "felicidad",
    "游땍": "felicidad",
    "游땕": "felicidad",
    "游댠": "felicidad",
    "游녨": "felicidad",
    "游꿀": "felicidad",
    "游": "felicidad",
    "游꾹": "felicidad",
    "游깯": "felicidad",
    "游눆": "felicidad",
    "游뗿": "felicidad",
    "游눝": "felicidad",
    "游녪游땘": "felicidad",
    "仇벒잺": "amor",
    "游땘": "amor",
    "游똂": "amor",
    "游눗": "amor",
    "游땢": "amor",
    "游": "admiraci칩n",
    "游눠": "admiraci칩n",
    "游눮": "admiraci칩n",
    "游땰": "tristeza",
    "游땩":"tristeza"
}

# Funci칩n para asignar categor칤a a cada emoji
def asignar_categoria(emoji):
    return categorias_emojis.get(emoji, "Desconocida")  # Retorna "Desconocida" si el emoji no est치 en el diccionario

# Aplicar la funci칩n a tu DataFrame
df_conteo_emojis['Categoria'] = df_conteo_emojis['Emoji'].apply(asignar_categoria)

df_agrupado = df_conteo_emojis.groupby('Categoria').sum()
df_agrupado = df_agrupado.sort_values(by='Frecuencia', ascending=False)


print(df_agrupado)

desconocidos = df_conteo_emojis[df_conteo_emojis['Categoria'] == 'Desconocida']
print(desconocidos)

# Filtrar para excluir categor칤a "Desconocida"
df_filtrado = df_conteo_emojis[df_conteo_emojis['Categoria'] != 'Desconocida']

# Agrupar por categor칤a y sumar frecuencias
df_agrupado = df_filtrado.groupby('Categoria')['Frecuencia'].sum()

# Generar el gr치fico
plt.figure(figsize=(10, 8))
plt.pie(df_agrupado, labels=df_agrupado.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Distribuci칩n de Emojis por Categor칤a (Excluyendo Desconocidos)')
plt.show()

"""descargar el dataframe descargar el resultado de la columna "sentimiento_bert_modificado" del DataFrame df_pandas"""

# Guardar la columna "sentimiento_bert_modificado" en un archivo CSV
df_pandas[['comentario', 'sentimiento_bert_modificado']].to_csv('archivo.csv', index=False)

# Descargar el archivo CSV
files.download('archivo.csv')

"""ADIESTRAMIENTO"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report


uploaded = files.upload()
data = pd.read_csv('archivo.csv')

# Preprocesamiento de datos: Dividir los datos en conjuntos de entrenamiento y de prueba
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Cargar el modelo BERT y el tokenizador
model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: positive, negative, neutral

# Tokenizando el texto
def tokenize_text(text_list):
    tokens = tokenizer.batch_encode_plus(text_list, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_tensors="tf")
    return {
        'input_ids': tokens['input_ids'],
        'attention_mask': tokens['attention_mask'],
        'token_type_ids': tokens['token_type_ids']
    }

# Preprocesamos el texto
max_length = 128  # Definimos una longitud para el texto
X_train = tokenize_text(train_data['comentario'].tolist())
X_test = tokenize_text(test_data['comentario'].tolist())
y_train = train_data['sentimiento_bert_modificado']
y_test = test_data['sentimiento_bert_modificado']

# Convertirmos las etiquetas en num칠ricas
label_map = {"positivo": 0, "negativo": 1, "neutro": 2}
y_train = [label_map[label] for label in y_train]
y_test = [label_map[label] for label in y_test]

# Preparamos el conjunto de entrenamiento
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# Definimos la funci칩n de p칠rdida, el optimizador y la m칠trica de evaluaci칩n
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
metrics = ['accuracy']

# Compilamos el modelo
bert_model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)

# Entrenamo el modelo
epochs = 3
batch_size = 32
history = bert_model.fit(train_dataset.batch(batch_size), epochs=epochs, validation_data=test_dataset.batch(batch_size))

# Evaluamos el modelo
y_pred = bert_model.predict(test_dataset.batch(batch_size))
y_pred_probabilities = tf.nn.softmax(y_pred.logits, axis=-1).numpy()  # Extraer probabilidades de clase mediante softmax
y_pred_classes = y_pred_probabilities.argmax(axis=1)  #Encontrar la clase con mayor probabilidad
accuracy = accuracy_score(y_test, y_pred_classes)
classification_rep = classification_report(y_test, y_pred_classes, target_names=label_map.keys())

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_rep)

# Guardar el modelo entrenado para uso futuro
bert_model.save("bert_sentiment_model")

import tensorflow as tf

# Carga el modelo entrenado
model = tf.keras.models.load_model("bert_sentiment_model")

"""NIVEL 1 verificmos si el modelo funciona"""

from transformers import BertTokenizer

model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

texto = input("Ingrese una oraci칩n para el an치lisis de sentimiento: ")

# Tokeniza el texto
tokens = tokenizer.encode_plus(texto, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors="tf")

# Realiza la predicci칩n
y_pred = model.predict({
    'input_ids': tokens['input_ids'],
    'attention_mask': tokens['attention_mask'],
    'token_type_ids': tokens['token_type_ids']
})

# Obt칠n las probabilidades de clase utilizando softmax
y_pred_logits = y_pred['logits']
y_pred_probabilities = tf.nn.softmax(y_pred_logits, axis=-1).numpy()

# Encuentra la clase con la probabilidad m치s alta
y_pred_class = y_pred_probabilities.argmax(axis=1)[0]

# Mapeo inverso de 칤ndice de clase a etiqueta de sentimiento
label_map_inverse = {0: "positivo", 1: "negativo", 2: "neutro"}

# Obtiene la etiqueta de sentimiento
sentimiento_predicho = label_map_inverse[y_pred_class]

# Imprime la etiqueta de sentimiento predicha
print("Sentimiento predicho:", sentimiento_predicho)

"""NIVEL 2. ahora si viene lo bueno"""

# Cargar el modelo entrenado
model = tf.keras.models.load_model("bert_sentiment_model")

# Cargar el tokenizador
model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

# Cargue un archivo Excel
uploaded = files.upload()

# Leer el archivo Excel y tokenizar los comentarios
if len(uploaded) > 0:
    file_name = list(uploaded.keys())[0]
    df = pd.read_excel(file_name)

    # Verificar si la columna 'comentario' existe en el archivo Excel
    if 'comentario' in df.columns:
        tokenized_comments = []

        for comentario in df['comentario']:
            tokens = tokenizer.encode_plus(comentario, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors="tf")
            tokenized_comments.append(tokens)

        # Realizar las predicciones
        predictions = []

        for tokens in tokenized_comments:
            y_pred = model.predict({
                'input_ids': tokens['input_ids'],
                'attention_mask': tokens['attention_mask'],
                'token_type_ids': tokens['token_type_ids']
            })

            # Obtener las probabilidades de clase utilizando softmax
            y_pred_probabilities = tf.nn.softmax(y_pred['logits'], axis=-1).numpy()

            # Encontrar la clase con la probabilidad m치s alta
            y_pred_class = y_pred_probabilities.argmax(axis=1)[0]

            # Mapeo inverso de 칤ndice de clase a etiqueta de sentimiento
            label_map_inverse = {0: "positivo", 1: "negativo", 2: "neutro"}

            # Obtener la etiqueta de sentimiento predicha
            sentimiento_predicho = label_map_inverse[y_pred_class]
            predictions.append(sentimiento_predicho)

        # Agregar las predicciones al DataFrame
        df['sentimiento_predicho'] = predictions

        # Generar un pie chart de las distribuciones de las etiquetas de sentimiento
        counts = df['sentimiento_predicho'].value_counts()
        labels = counts.index.tolist()
        sizes = counts.values

        # Configurar el gr치fico circular
        plt.figure(figsize=(8, 8))
        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
        plt.title('Distribuci칩n de Sentimientos Predichos')

        # Mostrar el gr치fico circular
        plt.show()
    else:
        print("El archivo Excel no contiene una columna llamada 'comentario'.")
else:
    print("No se carg칩 ning칰n archivo Excel.")

# Realizar las predicciones y obtener las probabilidades de clase
predicted_probabilities = []

for tokens in tokenized_comments:
    y_pred = model.predict({
        'input_ids': tokens['input_ids'],
        'attention_mask': tokens['attention_mask'],
        'token_type_ids': tokens['token_type_ids']
    })

    # Obtener las probabilidades de clase utilizando softmax
    y_pred_probabilities = tf.nn.softmax(y_pred['logits'], axis=-1).numpy()
    predicted_probabilities.append(y_pred_probabilities)

# Crear un DataFrame con las probabilidades de clase
df['predicted_probabilities'] = predicted_probabilities

# Agregar las predicciones al DataFrame
df['sentimiento_predicho'] = predictions

# Mostrar el DataFrame con las predicciones
print(df)

# Crear un diccionario de mapeo de etiquetas a valores num칠ricos
etiqueta_a_valor = {"positivo": 0, "negativo": 1, "neutro": 2}

# Agregar una nueva columna "puntuaci칩n" basada en la columna existente "sentimiento"
df['puntuaci칩n'] = df['sentimiento_predicho'].map(etiqueta_a_valor)

# Imprimir los nombres de las columnas en el DataFrame
print(df.columns)

print (df)

import matplotlib.pyplot as plt

# Resumen estad칤stico de la columna 'puntuaci칩n'
print(df['puntuaci칩n'].describe())

# Histograma de la columna 'puntuaci칩n'
plt.hist(df['puntuaci칩n'], bins=[0, 1, 2, 3], edgecolor='k', alpha=0.7)
plt.xlabel('Puntuaci칩n')
plt.ylabel('Frecuencia')
plt.xticks([0, 1, 2], ['Positivo', 'Negativo', 'Neutro'])
plt.title('Distribuci칩n de Puntuaciones')
plt.show()