# -*- coding: utf-8 -*-
"""entrenado desde UAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MkWqPJXFzxsJOJMN7ZrwjyHLrHOyoF5g
"""

!pip install pandas nltk transformers matplotlib pyspark emoji textblob openpyxl

import nltk
import emoji
import string
import re
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from transformers import pipeline
from wordcloud import WordCloud
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from collections import Counter
from google.colab import files

# Crear SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

try:
    nltk.download('vader_lexicon')
    nltk.download('punkt')
except Exception as e:
    print("Error al descargar recursos de NLTK:", e)

# Cargar el modelo BERT para español
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
# Función para análisis de sentimientos con NLTK
def analizar_sentimiento_nltk(texto):
    sia = SentimentIntensityAnalyzer()
    texto_modificado = texto.replace("ja", "feliz").replace("XD", "alegre")
    puntaje = sia.polarity_scores(texto_modificado)
    return 'positivo' if puntaje['compound'] > 0.05 else ('negativo' if puntaje['compound'] < -0.05 else 'neutro')
# Función para análisis de sentimientos con BERT
def analizar_sentimiento_bert(texto):
    resultado = sentiment_pipeline(texto)[0]
    return resultado['label']
# UDFs para Spark
analizar_sentimiento_nltk_udf = udf(analizar_sentimiento_nltk, StringType())
analizar_sentimiento_bert_udf = udf(analizar_sentimiento_bert, StringType())

# Subir archivo
uploaded = files.upload()
nombre_archivo = next(iter(uploaded))

# Leer el archivo xls en un DataFrame de Spark
df_pandas = pd.read_excel(nombre_archivo)

# Función para extraer emojis de un texto usando una expresión regular
def extraer_emojis(texto):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return ' '.join(emoji_pattern.findall(texto))

# Aplicar la función para extraer emojis a cada comentario
df_pandas['emojis'] = df_pandas['comentario'].apply(extraer_emojis)

# Crear un DataFrame solo con los comentarios y los emojis extraídos
df_emojis = df_pandas[['comentario', 'emojis']]

# Filtrar para mostrar solo filas donde se encontraron emojis
df_emojis = df_emojis[df_emojis['emojis'].str.len() > 0]

# Mostrar el DataFrame resultante
print(df_emojis)

# Crear una lista con todos los emojis
lista_emojis = ' '.join(df_emojis['emojis']).split()

# Contar la frecuencia de cada emoji
conteo_emojis = Counter(lista_emojis)

# Convertir el DataFrame de Pandas a un DataFrame de Spark
df = spark.createDataFrame(df_pandas)

# Aplicar UDFs al DataFrame
df = df.withColumn("sentimiento_nltk", analizar_sentimiento_nltk_udf(col("comentario")))
df = df.withColumn("sentimiento_bert", analizar_sentimiento_bert_udf(col("comentario")))

df.show()

# Convertir a DataFrame de Pandas para facilitar la visualización
df_pandas = df.toPandas()

# Contar sentimientos para NLTK y BERT
sentimientos_nltk = df_pandas['sentimiento_nltk'].value_counts()
sentimientos_bert = df_pandas['sentimiento_bert'].value_counts()

# Crear gráficos de barras
plt.figure(figsize=(12, 12))

# Gráfico para NLTK
plt.subplot(1, 2, 1)
sentimientos_nltk.plot(kind='bar', color=['green', 'blue', 'red'])
plt.title('Análisis de Sentimientos NLTK')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad')

# Gráfico para BERT
plt.subplot(1, 2, 2)
sentimientos_bert.plot(kind='bar', color=['green', 'blue', 'red'])
plt.title('Análisis de Sentimientos BERT')
plt.xlabel('Sentimiento')
plt.ylabel('Cantidad')


plt.tight_layout()
plt.show()

# Función para limpiar y tokenizar el texto
def limpiar_y_tokenizar(texto):
    # Convertir a minúsculas
    texto = texto.lower()
    # Eliminar puntuación
    texto = texto.translate(str.maketrans('', '', string.punctuation))
    # Tokenizar
    tokens = word_tokenize(texto)
    return tokens

# Aplicar la función de limpieza y tokenización a cada comentario
tokens = df_pandas['comentario'].apply(limpiar_y_tokenizar)

# Contar la frecuencia de cada palabra
frecuencia_palabras = Counter()
for comentario in tokens:
    frecuencia_palabras.update(comentario)

# Convertir el conteo de palabras en un DataFrame de Pandas
df_frecuencia_palabras = pd.DataFrame(frecuencia_palabras.items(), columns=['Palabra', 'Frecuencia']).sort_values(by='Frecuencia', ascending=False)

# Mostrar las 20 palabras más comunes en un cuadro
df_frecuencia_palabras.head(20)

# Función para contar sílabas en español (aproximación simple)
def contar_silabas(palabra):
    # Contar vocales en la palabra
    return sum(1 for letra in palabra if letra in "aeiouáéíóú")

# Filtrar palabras con más de dos sílabas, excepto 'u'
palabras_filtradas = {palabra: frecuencia for palabra, frecuencia in frecuencia_palabras.items() if contar_silabas(palabra) > 2 or palabra == 'u'}

# Crear una nube de palabras
nube_palabras = WordCloud(width=800, height=400, background_color ='white').generate_from_frequencies(palabras_filtradas)

# Mostrar la nube de palabras
plt.figure(figsize=(10, 5))
plt.imshow(nube_palabras, interpolation='bilinear')
plt.axis('off')
plt.show()

# Calcular la longitud de cada comentario
df_pandas['longitud'] = df_pandas['comentario'].apply(len)

# Crear gráficos de dispersión
plt.figure(figsize=(14, 6))

# Gráfico para NLTK
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_nltk')
plt.title('Longitud de Comentarios vs Sentimientos (NLTK)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

# Gráfico para BERT
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_bert')
plt.title('Longitud de Comentarios vs Sentimientos (BERT)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

plt.tight_layout()
plt.show()

# Función modificada para análisis de sentimientos con BERT
def analizar_sentimiento_bert_modificado(texto):
    resultado = sentiment_pipeline(texto)[0]
    etiqueta = resultado['label']

    # Convertir la clasificación de estrellas a positivo/negativo/neutro
    if etiqueta in ["1 star", "2 stars"]:
        return "negativo"
    elif etiqueta == "3 stars":
        return "neutro"
    else:  # "4 stars" y "5 stars"
        return "positivo"

# Aplicar la función modificada al DataFrame
df_pandas['sentimiento_bert_modificado'] = df_pandas['comentario'].apply(analizar_sentimiento_bert_modificado)

# Crear gráficos de dispersión
plt.figure(figsize=(14, 6))

# Gráfico para NLTK
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_nltk')
plt.title('Longitud de Comentarios vs Sentimientos (NLTK)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

# Gráfico para BERT modificado
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_pandas, x='longitud', y='sentimiento_bert_modificado')
plt.title('Longitud de Comentarios vs Sentimientos (BERT Modificado)')
plt.xlabel('Longitud del Comentario')
plt.ylabel('Sentimiento')

plt.tight_layout()
plt.show()

# Categorizar la longitud de los comentarios
def categorizar_longitud(longitud):
    if longitud < 50:
        return 'Corto'
    elif longitud < 100:
        return 'Medio'
    else:
        return 'Largo'

df_pandas['categoria_longitud'] = df_pandas['longitud'].apply(categorizar_longitud)

# Contar sentimientos por categoría de longitud para NLTK y BERT
sentimientos_por_longitud_nltk = df_pandas.groupby(['categoria_longitud', 'sentimiento_nltk']).size().unstack().fillna(0)
sentimientos_por_longitud_bert = df_pandas.groupby(['categoria_longitud', 'sentimiento_bert']).size().unstack().fillna(0)

# Crear gráficos de barras apiladas
plt.figure(figsize=(14, 6))

# Gráfico para NLTK
plt.subplot(1, 2, 1)
sentimientos_por_longitud_nltk.plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Sentimientos por Longitud de Comentarios (NLTK)')
plt.xlabel('Categoría de Longitud')
plt.ylabel('Cantidad')
plt.legend(title='Sentimiento')

# Gráfico para BERT
plt.subplot(1, 2, 2)
sentimientos_por_longitud_bert.plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Sentimientos por Longitud de Comentarios (BERT)')
plt.xlabel('Categoría de Longitud')
plt.ylabel('Cantidad')
plt.legend(title='Sentimiento')

plt.tight_layout()
plt.show()

# Crear un nuevo DataFrame
bert_sentimientos = df_pandas[['comentario', 'sentimiento_bert_modificado']]

# Mostrar el nuevo DataFrame
print(bert_sentimientos)

# Convertir el conteo en un DataFrame
df_conteo_emojis = pd.DataFrame(conteo_emojis.items(), columns=['Emoji', 'Frecuencia'])

# Ordenar el DataFrame por frecuencia de manera descendente
df_conteo_emojis = df_conteo_emojis.sort_values(by='Frecuencia', ascending=False)

# Configurar pandas para mostrar todas las filas
pd.set_option('display.max_rows', None)

# Mostrar el DataFrame
print(df_conteo_emojis)

print(df_conteo_emojis)


categorias_emojis = {
    "👏": "felicidad",
    "😂": "felicidad",
    "😊": "felicidad",
    "🔥": "felicidad",
    "👍": "felicidad",
    "🎉": "felicidad",
    "🌟": "felicidad",
    "🎂": "felicidad",
    "🌈": "felicidad",
    "💃": "felicidad",
    "🙌": "felicidad",
    "💛": "felicidad",
    "👏😍": "felicidad",
    "❤️": "amor",
    "😍": "amor",
    "🙏": "amor",
    "💕": "amor",
    "😘": "amor",
    "👀": "admiración",
    "💡": "admiración",
    "💯": "admiración",
    "😩": "tristeza",
    "😢":"tristeza"
}

# Función para asignar categoría a cada emoji
def asignar_categoria(emoji):
    return categorias_emojis.get(emoji, "Desconocida")  # Retorna "Desconocida" si el emoji no está en el diccionario

# Aplicar la función a tu DataFrame
df_conteo_emojis['Categoria'] = df_conteo_emojis['Emoji'].apply(asignar_categoria)

df_agrupado = df_conteo_emojis.groupby('Categoria').sum()
df_agrupado = df_agrupado.sort_values(by='Frecuencia', ascending=False)


print(df_agrupado)

desconocidos = df_conteo_emojis[df_conteo_emojis['Categoria'] == 'Desconocida']
print(desconocidos)

# Filtrar para excluir categoría "Desconocida"
df_filtrado = df_conteo_emojis[df_conteo_emojis['Categoria'] != 'Desconocida']

# Agrupar por categoría y sumar frecuencias
df_agrupado = df_filtrado.groupby('Categoria')['Frecuencia'].sum()

# Generar el gráfico
plt.figure(figsize=(10, 8))
plt.pie(df_agrupado, labels=df_agrupado.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')
plt.title('Distribución de Emojis por Categoría (Excluyendo Desconocidos)')
plt.show()

"""descargar el dataframe descargar el resultado de la columna "sentimiento_bert_modificado" del DataFrame df_pandas"""

# Guardar la columna "sentimiento_bert_modificado" en un archivo CSV
df_pandas[['comentario', 'sentimiento_bert_modificado']].to_csv('archivo.csv', index=False)

# Descargar el archivo CSV
files.download('archivo.csv')

"""ADIESTRAMIENTO"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report


uploaded = files.upload()
data = pd.read_csv('archivo.csv')

# Preprocesamiento de datos: Dividir los datos en conjuntos de entrenamiento y de prueba
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Cargar el modelo BERT y el tokenizador
model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: positive, negative, neutral

# Tokenizando el texto
def tokenize_text(text_list):
    tokens = tokenizer.batch_encode_plus(text_list, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_tensors="tf")
    return {
        'input_ids': tokens['input_ids'],
        'attention_mask': tokens['attention_mask'],
        'token_type_ids': tokens['token_type_ids']
    }

# Preprocesamos el texto
max_length = 128  # Definimos una longitud para el texto
X_train = tokenize_text(train_data['comentario'].tolist())
X_test = tokenize_text(test_data['comentario'].tolist())
y_train = train_data['sentimiento_bert_modificado']
y_test = test_data['sentimiento_bert_modificado']

# Convertirmos las etiquetas en numéricas
label_map = {"positivo": 0, "negativo": 1, "neutro": 2}
y_train = [label_map[label] for label in y_train]
y_test = [label_map[label] for label in y_test]

# Preparamos el conjunto de entrenamiento
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

# Definimos la función de pérdida, el optimizador y la métrica de evaluación
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
metrics = ['accuracy']

# Compilamos el modelo
bert_model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)

# Entrenamo el modelo
epochs = 3
batch_size = 32
history = bert_model.fit(train_dataset.batch(batch_size), epochs=epochs, validation_data=test_dataset.batch(batch_size))

# Evaluamos el modelo
y_pred = bert_model.predict(test_dataset.batch(batch_size))
y_pred_probabilities = tf.nn.softmax(y_pred.logits, axis=-1).numpy()  # Extraer probabilidades de clase mediante softmax
y_pred_classes = y_pred_probabilities.argmax(axis=1)  #Encontrar la clase con mayor probabilidad
accuracy = accuracy_score(y_test, y_pred_classes)
classification_rep = classification_report(y_test, y_pred_classes, target_names=label_map.keys())

print("Accuracy:", accuracy)
print("Classification Report:\n", classification_rep)

# Guardar el modelo entrenado para uso futuro
bert_model.save("bert_sentiment_model")

import tensorflow as tf

# Carga el modelo entrenado
model = tf.keras.models.load_model("bert_sentiment_model")

"""NIVEL 1 verificmos si el modelo funciona"""

from transformers import BertTokenizer

model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

texto = input("Ingrese una oración para el análisis de sentimiento: ")

# Tokeniza el texto
tokens = tokenizer.encode_plus(texto, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors="tf")

# Realiza la predicción
y_pred = model.predict({
    'input_ids': tokens['input_ids'],
    'attention_mask': tokens['attention_mask'],
    'token_type_ids': tokens['token_type_ids']
})

# Obtén las probabilidades de clase utilizando softmax
y_pred_logits = y_pred['logits']
y_pred_probabilities = tf.nn.softmax(y_pred_logits, axis=-1).numpy()

# Encuentra la clase con la probabilidad más alta
y_pred_class = y_pred_probabilities.argmax(axis=1)[0]

# Mapeo inverso de índice de clase a etiqueta de sentimiento
label_map_inverse = {0: "positivo", 1: "negativo", 2: "neutro"}

# Obtiene la etiqueta de sentimiento
sentimiento_predicho = label_map_inverse[y_pred_class]

# Imprime la etiqueta de sentimiento predicha
print("Sentimiento predicho:", sentimiento_predicho)

"""NIVEL 2. ahora si viene lo bueno"""

# Cargar el modelo entrenado
model = tf.keras.models.load_model("bert_sentiment_model")

# Cargar el tokenizador
model_name = "bert-base-multilingual-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

# Cargue un archivo Excel
uploaded = files.upload()

# Leer el archivo Excel y tokenizar los comentarios
if len(uploaded) > 0:
    file_name = list(uploaded.keys())[0]
    df = pd.read_excel(file_name)

    # Verificar si la columna 'comentario' existe en el archivo Excel
    if 'comentario' in df.columns:
        tokenized_comments = []

        for comentario in df['comentario']:
            tokens = tokenizer.encode_plus(comentario, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors="tf")
            tokenized_comments.append(tokens)

        # Realizar las predicciones
        predictions = []

        for tokens in tokenized_comments:
            y_pred = model.predict({
                'input_ids': tokens['input_ids'],
                'attention_mask': tokens['attention_mask'],
                'token_type_ids': tokens['token_type_ids']
            })

            # Obtener las probabilidades de clase utilizando softmax
            y_pred_probabilities = tf.nn.softmax(y_pred['logits'], axis=-1).numpy()

            # Encontrar la clase con la probabilidad más alta
            y_pred_class = y_pred_probabilities.argmax(axis=1)[0]

            # Mapeo inverso de índice de clase a etiqueta de sentimiento
            label_map_inverse = {0: "positivo", 1: "negativo", 2: "neutro"}

            # Obtener la etiqueta de sentimiento predicha
            sentimiento_predicho = label_map_inverse[y_pred_class]
            predictions.append(sentimiento_predicho)

        # Agregar las predicciones al DataFrame
        df['sentimiento_predicho'] = predictions

        # Generar un pie chart de las distribuciones de las etiquetas de sentimiento
        counts = df['sentimiento_predicho'].value_counts()
        labels = counts.index.tolist()
        sizes = counts.values

        # Configurar el gráfico circular
        plt.figure(figsize=(8, 8))
        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
        plt.title('Distribución de Sentimientos Predichos')

        # Mostrar el gráfico circular
        plt.show()
    else:
        print("El archivo Excel no contiene una columna llamada 'comentario'.")
else:
    print("No se cargó ningún archivo Excel.")

# Realizar las predicciones y obtener las probabilidades de clase
predicted_probabilities = []

for tokens in tokenized_comments:
    y_pred = model.predict({
        'input_ids': tokens['input_ids'],
        'attention_mask': tokens['attention_mask'],
        'token_type_ids': tokens['token_type_ids']
    })

    # Obtener las probabilidades de clase utilizando softmax
    y_pred_probabilities = tf.nn.softmax(y_pred['logits'], axis=-1).numpy()
    predicted_probabilities.append(y_pred_probabilities)

# Crear un DataFrame con las probabilidades de clase
df['predicted_probabilities'] = predicted_probabilities

# Agregar las predicciones al DataFrame
df['sentimiento_predicho'] = predictions

# Mostrar el DataFrame con las predicciones
print(df)

# Crear un diccionario de mapeo de etiquetas a valores numéricos
etiqueta_a_valor = {"positivo": 0, "negativo": 1, "neutro": 2}

# Agregar una nueva columna "puntuación" basada en la columna existente "sentimiento"
df['puntuación'] = df['sentimiento_predicho'].map(etiqueta_a_valor)

# Imprimir los nombres de las columnas en el DataFrame
print(df.columns)

print (df)

import matplotlib.pyplot as plt

# Resumen estadístico de la columna 'puntuación'
print(df['puntuación'].describe())

# Histograma de la columna 'puntuación'
plt.hist(df['puntuación'], bins=[0, 1, 2, 3], edgecolor='k', alpha=0.7)
plt.xlabel('Puntuación')
plt.ylabel('Frecuencia')
plt.xticks([0, 1, 2], ['Positivo', 'Negativo', 'Neutro'])
plt.title('Distribución de Puntuaciones')
plt.show()